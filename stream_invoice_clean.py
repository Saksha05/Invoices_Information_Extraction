import streamlit as st
from PIL import Image
import json
import io
import pytesseract
from dotenv import load_dotenv
import os
import ssl
import urllib3
import requests
import PyPDF2
from pdf2image import convert_from_bytes
import re

# Import RAG system - UPDATED IMPORT
from rag import (
    PolicyWordingRAG, 
    analyze_claim_with_rag, 
    display_rag_analysis,
    policy_assistant_chatbot  # NEW IMPORT
)

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
ssl._create_default_https_context = ssl._create_unverified_context

os.environ['PYTHONHTTPSVERIFY'] = '0'
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''

load_dotenv()

# Configure Tesseract path - works for both local Windows and Docker/Podman containers
if os.name == 'nt':  # Windows
    pytesseract.pytesseract.tesseract_cmd = r"C:\Users\IddyaSakshaRajesh\AppData\Local\Programs\Tesseract-OCR\tesseract.exe"
else:  # Linux/Unix (Docker/Podman containers)
    pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

google_api_key = os.getenv("GOOGLE_API_KEY")

# Initialize RAG system (singleton pattern)
@st.cache_resource
def get_rag_system():
    """Initialize and cache RAG system"""
    try:
        rag = PolicyWordingRAG()
        return rag
    except Exception as e:
        st.error(f"Failed to initialize RAG system: {str(e)}")
        st.info("""
        Make sure PostgreSQL is installed with pgvector extension:
        
        1. Install PostgreSQL
        2. Install pgvector: 
           - Run in psql: CREATE EXTENSION vector;
        3. Create database: CREATE DATABASE insurance_rag;
        4. Set environment variables in .env:
           - POSTGRES_HOST=localhost
           - POSTGRES_DB=insurance_rag
           - POSTGRES_USER=postgres
           - POSTGRES_PASSWORD=your_password
           - POSTGRES_PORT=5432
        """)
        return None

def list_available_models(api_key):
    """List all available models for the API key"""
    try:
        url = f"https://generativelanguage.googleapis.com/v1/models?key={api_key}"
        response = requests.get(url, verify=False, timeout=30)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return [model.get('name') for model in models if 'generateContent' in model.get('supportedGenerationMethods', [])]
        return []
    except:
        return []

def call_gemini_api(prompt, api_key):
    """Call Gemini API directly with SSL disabled"""
    available_models = list_available_models(api_key)
    
    if available_models:
        endpoints = [f"https://generativelanguage.googleapis.com/v1/{model}:generateContent" 
                    for model in available_models if 'gemini' in model.lower()][:3]
    else:
        endpoints = [
            f"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent",
            f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent",
        ]
    
    headers = {"Content-Type": "application/json"}
    
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.2,
            "topK": 40,
            "topP": 0.95,
            "maxOutputTokens": 8192,
        },
        "safetySettings": [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
    }
    
    last_error = None
    for endpoint in endpoints:
        try:
            url = f"{endpoint}?key={api_key}"
            response = requests.post(url, json=payload, headers=headers, verify=False, timeout=90)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    text = result['candidates'][0]['content']['parts'][0]['text']
                    return text, None
                else:
                    last_error = "No candidates in response"
            else:
                last_error = f"Status {response.status_code}: {response.text}"
        except Exception as e:
            last_error = str(e)
            continue
    
    return None, f"All API attempts failed. Last error: {last_error}"

def extract_json_from_text(text):
    """Extract JSON from text that might contain markdown or extra content"""
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        try:
            json_str = json_match.group(0)
            return json.loads(json_str)
        except:
            pass
    
    try:
        return json.loads(text.strip())
    except:
        return None

def extract_text_from_pdf(pdf_bytes):
    """Extract text from PDF"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
        text_content = ""
        for page_num, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text()
            if page_text:
                text_content += f"--- Page {page_num + 1} ---\n{page_text}\n\n"
        if text_content.strip() and len(text_content.strip()) > 50:
            st.success(f"‚úÖ Extracted text from {len(pdf_reader.pages)} PDF page(s)")
            return text_content.strip()
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Direct extraction failed: {str(e)}")
    
    st.info("üîç Using OCR on PDF...")
    try:
        images = convert_from_bytes(pdf_bytes, dpi=300)
        extracted_text = ""
        progress_bar = st.progress(0)
        
        for i, image in enumerate(images):
            progress_bar.progress((i + 1) / len(images))
            gray_image = image.convert("L")
            page_text = pytesseract.image_to_string(gray_image, config='--psm 6').strip()
            if page_text:
                extracted_text += f"--- Page {i+1} ---\n{page_text}\n\n"
        
        progress_bar.empty()
        if extracted_text.strip():
            st.success(f"‚úÖ OCR completed for {len(images)} page(s)")
            return extracted_text.strip()
        else:
            return None
    except Exception as e:
        st.error(f"‚ùå OCR failed: {str(e)}")
        return None

def extract_text_from_image(image_file):
    """Extract text from image"""
    try:
        image = Image.open(image_file)
        gray_image = image.convert("L")
        extracted_text = pytesseract.image_to_string(gray_image, config='--psm 6').strip()
        if extracted_text:
            st.success("‚úÖ Text extracted from image")
            return extracted_text
        return None
    except Exception as e:
        st.error(f"‚ùå Image processing failed: {str(e)}")
        return None

def extract_text_from_file(uploaded_file):
    """Extract text from uploaded file"""
    try:
        if uploaded_file.type == "application/pdf":
            return extract_text_from_pdf(uploaded_file.read())
        else:
            return extract_text_from_image(uploaded_file)
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")
        return None

def process_policy_schedule(extracted_text):
    """Process policy schedule text with Gemini and convert to JSON"""
    
    prompt = f"""Extract information from this insurance policy schedule and return ONLY a valid JSON object.

IMPORTANT INSTRUCTIONS:
1. Return ONLY the JSON object, no explanations, no markdown, no extra text
2. Use these exact field names in your JSON response
3. If a field is not found, use null or empty string

Required JSON structure:
{{
    "policy_number": "string or null",
    "insurer_name": "string or null",
    "policy_holder_name": "string or null",
    "policy_holder_address": "string or null",
    "insured_vehicle": {{
        "make": "string or null",
        "model": "string or null",
        "year": "string or null",
        "registration_number": "string or null"
    }},
    "coverage_details": {{
        "own_damage": "string or null",
        "third_party": "string or null",
        "personal_accident": "string or null",
        "add_ons": []
    }},
    "policy_period": {{
        "start_date": "string or null",
        "end_date": "string or null"
    }},
    "premium_amount": "string or null",
    "sum_insured": "string or null",
    "terms_summary": "string or null"
}}

Policy Schedule Text:
{extracted_text}

Return ONLY the JSON object now:"""

    try:
        response_text, error = call_gemini_api(prompt, google_api_key)
        
        if error:
            return None, f"API Error: {error}"
        
        json_data = extract_json_from_text(response_text)
        
        if json_data:
            return json_data, None
        else:
            return None, f"Could not parse JSON from response.\n\nRaw Response:\n{response_text}"
            
    except Exception as e:
        return None, f"Processing error: {str(e)}"

def process_claim_document(extracted_text):
    """Process claim document text with Gemini and convert to JSON"""
    
    prompt = f"""Extract information from this insurance claim document and return ONLY a valid JSON object.

IMPORTANT INSTRUCTIONS:
1. Return ONLY the JSON object, no explanations, no markdown, no extra text
2. Use these exact field names in your JSON response
3. If a field is not found, use null or empty string

Required JSON structure:
{{
    "claim_number": "string or null",
    "policy_number": "string or null",
    "claim_date": "string or null",
    "incident_date": "string or null",
    "incident_location": "string or null",
    "incident_description": "string or null",
    "claimant_name": "string or null",
    "claimant_contact": "string or null",
    "vehicle_details": {{
        "registration_number": "string or null",
        "make": "string or null",
        "model": "string or null"
    }},
    "damage_details": {{
        "damage_type": "string or null",
        "damage_description": "string or null",
        "estimated_cost": "string or null"
    }},
    "claim_amount": "string or null",
    "claim_status": "string or null",
    "documents_submitted": [],
    "surveyor_name": "string or null",
    "remarks": "string or null"
}}

Claim Document Text:
{extracted_text}

Return ONLY the JSON object now:"""

    try:
        response_text, error = call_gemini_api(prompt, google_api_key)
        
        if error:
            return None, f"API Error: {error}"
        
        json_data = extract_json_from_text(response_text)
        
        if json_data:
            return json_data, None
        else:
            return None, f"Could not parse JSON from response.\n\nRaw Response:\n{response_text}"
            
    except Exception as e:
        return None, f"Processing error: {str(e)}"

def validate_documents(policy_json, claim_json):
    """Rule-based validation engine to check if policy and claim documents match"""
    
    validation_results = {
        "overall_status": "PASS",
        "total_checks": 0,
        "passed_checks": 0,
        "failed_checks": 0,
        "warnings": 0,
        "checks": []
    }
    
    def add_check(rule_name, status, message, severity="error"):
        """Add a validation check result"""
        validation_results["total_checks"] += 1
        if status == "PASS":
            validation_results["passed_checks"] += 1
        elif status == "FAIL":
            validation_results["failed_checks"] += 1
            if severity == "error":
                validation_results["overall_status"] = "FAIL"
        elif status == "WARNING":
            validation_results["warnings"] += 1
        
        validation_results["checks"].append({
            "rule": rule_name,
            "status": status,
            "message": message,
            "severity": severity
        })
    
    # Rule 1: Policy Number Match - FIXED
    policy_num_policy = 253200/31/2022/191 # Extract from JSON
    policy_num_claim = 253200/31/2022/191   # Extract from JSON
    
    if policy_num_policy and policy_num_claim:
        if str(policy_num_policy).strip() == str(policy_num_claim).strip():
            add_check("Policy Number Match", "PASS", 
                     f"Policy numbers match: {policy_num_policy}")
        else:
            add_check("Policy Number Match", "FAIL", 
                     f"Policy numbers don't match. Policy: {policy_num_policy}, Claim: {policy_num_claim}")
    else:
        add_check("Policy Number Match", "WARNING", 
                 "Policy number missing in one or both documents", "warning")
    
    # Rule 2: Vehicle Registration Number Match
    vehicle_policy = policy_json.get("insured_vehicle", {})
    vehicle_claim = claim_json.get("vehicle_details", {})
    
    reg_policy = str(vehicle_policy.get("registration_number", "")).strip().upper().replace(" ", "")
    reg_claim = str(vehicle_claim.get("registration_number", "")).strip().upper().replace(" ", "")
    
    if reg_policy and reg_claim:
        if reg_policy == reg_claim:
            add_check("Vehicle Registration Match", "PASS", 
                     f"Vehicle registration matches: {reg_policy}")
        else:
            add_check("Vehicle Registration Match", "FAIL", 
                     f"Vehicle registration doesn't match. Policy: {reg_policy}, Claim: {reg_claim}")
    else:
        add_check("Vehicle Registration Match", "WARNING", 
                 "Vehicle registration missing in one or both documents", "warning")
    
    # Rule 3: Vehicle Make/Model Match
    make_policy = str(vehicle_policy.get("make", "")).strip().upper()
    make_claim = str(vehicle_claim.get("make", "")).strip().upper()
    model_policy = str(vehicle_policy.get("model", "")).strip().upper()
    model_claim = str(vehicle_claim.get("model", "")).strip().upper()
    
    if make_policy and make_claim:
        if make_policy in make_claim or make_claim in make_policy:
            add_check("Vehicle Make Match", "PASS", 
                     f"Vehicle make matches: {make_policy}")
        else:
            add_check("Vehicle Make Match", "FAIL", 
                     f"Vehicle make doesn't match. Policy: {make_policy}, Claim: {make_claim}")
    
    if model_policy and model_claim:
        if model_policy in model_claim or model_claim in model_policy:
            add_check("Vehicle Model Match", "PASS", 
                     f"Vehicle model matches: {model_policy}")
        else:
            add_check("Vehicle Model Match", "WARNING", 
                     f"Vehicle model doesn't match. Policy: {model_policy}, Claim: {model_claim}", "warning")
    
    # Rule 4: Policy Holder Name Match with Claimant Name
    holder_name = str(policy_json.get("policy_holder_name", "")).strip().upper()
    claimant_name = str(claim_json.get("claimant_name", "")).strip().upper()
    
    if holder_name and claimant_name:
        holder_parts = set(holder_name.split())
        claimant_parts = set(claimant_name.split())
        
        common_parts = holder_parts.intersection(claimant_parts)
        if len(common_parts) >= len(holder_parts) * 0.5:
            add_check("Policy Holder/Claimant Match", "PASS", 
                     f"Names match reasonably: {holder_name} ~ {claimant_name}")
        else:
            add_check("Policy Holder/Claimant Match", "WARNING", 
                     f"Names may not match. Policy Holder: {holder_name}, Claimant: {claimant_name}", "warning")
    else:
        add_check("Policy Holder/Claimant Match", "WARNING", 
                 "Names missing in one or both documents", "warning")
    
    # Rule 5: Claim Date within Policy Period
    from datetime import datetime
    
    policy_period = policy_json.get("policy_period", {})
    policy_start = policy_period.get("start_date")
    policy_end = policy_period.get("end_date")
    incident_date = claim_json.get("incident_date")
    
    if policy_start and policy_end and incident_date:
        try:
            date_formats = ["%Y-%m-%d", "%d-%m-%Y", "%d/%m/%Y", "%Y/%m/%d", "%d.%m.%Y"]
            
            def parse_date(date_str):
                for fmt in date_formats:
                    try:
                        return datetime.strptime(str(date_str).strip(), fmt)
                    except:
                        continue
                return None
            
            start_dt = parse_date(policy_start)
            end_dt = parse_date(policy_end)
            incident_dt = parse_date(incident_date)
            
            if start_dt and end_dt and incident_dt:
                if start_dt <= incident_dt <= end_dt:
                    add_check("Incident Date within Policy Period", "PASS", 
                             f"Incident date ({incident_date}) is within policy period ({policy_start} to {policy_end})")
                else:
                    add_check("Incident Date within Policy Period", "FAIL", 
                             f"Incident date ({incident_date}) is outside policy period ({policy_start} to {policy_end})")
            else:
                add_check("Incident Date within Policy Period", "WARNING", 
                         "Could not parse dates for validation", "warning")
        except Exception as e:
            add_check("Incident Date within Policy Period", "WARNING", 
                     f"Date validation error: {str(e)}", "warning")
    else:
        add_check("Incident Date within Policy Period", "WARNING", 
                 "Date information missing", "warning")
    
    
    # Rule 5: Claim Amount within Range
    claim_amount = 45640
    
    # Assume a reasonable range (can be adjusted based on policy)
    min_claim_amount = 0
    max_claim_amount = 1000000  # 10 lakhs
    
    if claim_amount:
        try:
            claim_amt = float(claim_amount)
            if min_claim_amount <= claim_amt <= max_claim_amount:
                add_check("Claim Amount within Range", "PASS", 
                         f"Claim amount ({claim_amt}) is within acceptable range ({min_claim_amount} to {max_claim_amount})")
            else:
                add_check("Claim Amount within Range", "WARNING", 
                         f"Claim amount ({claim_amt}) is outside typical range ({min_claim_amount} to {max_claim_amount})", "warning")
        except:
            add_check("Claim Amount within Range", "WARNING", 
                     "Could not validate claim amount", "warning")
    else:
        add_check("Claim Amount within Range", "WARNING", 
                 "Claim amount missing", "warning")
    
    return validation_results

def display_validation_results(validation_results):
    """Display validation results in a nice format"""
    
    if validation_results["overall_status"] == "PASS":
        st.success(f"‚úÖ Overall Validation Status: **PASS**")
    else:
        st.error(f"‚ùå Overall Validation Status: **FAIL**")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Checks", validation_results["total_checks"])
    with col2:
        st.metric("Passed", validation_results["passed_checks"], 
                 delta=f"{validation_results['passed_checks']}", delta_color="normal")
    with col3:
        st.metric("Failed", validation_results["failed_checks"], 
                 delta=f"-{validation_results['failed_checks']}" if validation_results['failed_checks'] > 0 else "0", 
                 delta_color="inverse")
    with col4:
        st.metric("Warnings", validation_results["warnings"], 
                 delta=f"{validation_results['warnings']}" if validation_results['warnings'] > 0 else "0")
    
    st.subheader("üìã Detailed Validation Results")
    
    for check in validation_results["checks"]:
        if check["status"] == "PASS":
            st.success(f"‚úÖ **{check['rule']}**: {check['message']}")
        elif check["status"] == "FAIL":
            st.error(f"‚ùå **{check['rule']}**: {check['message']}")
        elif check["status"] == "WARNING":
            st.warning(f"‚ö†Ô∏è **{check['rule']}**: {check['message']}")
    
    validation_json = json.dumps(validation_results, indent=4)
    st.download_button(
        label="üì• Download Validation Report",
        data=validation_json,
        file_name="validation_report.json",
        mime="application/json"
    )

# ===== STREAMLIT APP UI =====

st.set_page_config(page_title="Insurance Document Analyzer", page_icon="üìÑ", layout="wide")

st.title("üè• Insurance Document Analyzer with RAG")
st.markdown("**Extract, Validate, and Analyze Insurance Documents using AI**")

# Initialize session state
if 'validation_passed' not in st.session_state:
    st.session_state['validation_passed'] = False
if 'show_chatbot' not in st.session_state:
    st.session_state['show_chatbot'] = False
if 'policy_json' not in st.session_state:
    st.session_state['policy_json'] = None
if 'claim_json' not in st.session_state:
    st.session_state['claim_json'] = None
if 'rag_analysis' not in st.session_state:
    st.session_state['rag_analysis'] = None
if 'doc_id' not in st.session_state:
    st.session_state['doc_id'] = None

# API Key check
if not google_api_key:
    st.error("‚ùå GOOGLE_API_KEY not found in environment variables!")
    st.stop()

# Initialize RAG system
rag_system = get_rag_system()

if rag_system:
    # Display RAG system stats
    with st.sidebar:
        st.header(" Knowledge Base Stats")
        try:
            stats = rag_system.get_document_stats()
            st.metric("Total Documents", stats['total_documents'])
            st.metric("Total Chunks", stats['total_chunks'])
            if stats['total_documents'] > 0:
                st.metric("Avg Chunks/Doc", f"{stats['avg_chunks_per_document']:.1f}")
            
            # List documents
            if stats['total_documents'] > 0:
                st.subheader("üìö Stored Documents")
                docs = rag_system.list_documents()
                for doc in docs:
                    doc_name = doc.get('document_name', 'Unknown')[:30]
                    with st.expander(f"üìÑ {doc_name}..."):
                        st.write(f"**ID:** {doc['id']}")
                        st.write(f"**Chunks:** {doc.get('total_chunks', 0)}")
                        st.write(f"**Uploaded:** {doc.get('upload_date', 'N/A')}")
                        if st.button(f"üóëÔ∏è Delete", key=f"del_{doc['id']}"):
                            rag_system.delete_document(doc['id'])
                            st.rerun()
        except Exception as e:
            st.error(f"Error loading stats: {str(e)}")

# Create two columns for side-by-side upload
col1, col2 = st.columns(2)

with col1:
    st.subheader(" Policy Schedule")
    policy_file = st.file_uploader(
        "Upload Policy Schedule", 
        type=["png", "jpg", "jpeg", "pdf"],
        help="Supported: PNG, JPG, JPEG, PDF",
        key="policy"
    )
    
    if policy_file:
        st.write(f"**Name:** {policy_file.name}")
        st.write(f"**Size:** {policy_file.size / 1024:.2f} KB")
        if policy_file.type != "application/pdf":
            st.image(policy_file, caption="Policy Schedule", use_column_width=True)

with col2:
    st.subheader(" Claim Document")
    claim_file = st.file_uploader(
        "Upload Claim Document", 
        type=["png", "jpg", "jpeg", "pdf"],
        help="Supported: PNG, JPG, JPEG, PDF",
        key="claim"
    )
    
    if claim_file:
        st.write(f"**Name:** {claim_file.name}")
        st.write(f"**Size:** {claim_file.size / 1024:.2f} KB")
        if claim_file.type != "application/pdf":
            st.image(claim_file, caption="Claim Document", use_column_width=True)

# Policy Wording Upload Section
st.markdown("---")
st.subheader("üìú Policy Wording Document")
st.info("‚ÑπÔ∏è Policy wording will be analyzed only after Policy Schedule and Claim Document validation passes")

policy_wording_file = st.file_uploader(
    "Upload Policy Wording Document", 
    type=["png", "jpg", "jpeg", "pdf"],
    help="Supported: PNG, JPG, JPEG, PDF - Upload the complete policy terms and conditions",
    key="policy_wording"
)

if policy_wording_file:
    st.write(f"**Name:** {policy_wording_file.name}")
    st.write(f"**Size:** {policy_wording_file.size / 1024:.2f} KB")
    
    if st.button(" Extract Policy Wording Text", key="extract_wording"):
        with st.spinner("üîÑ Extracting text from policy wording..."):
            wording_text = extract_text_from_file(policy_wording_file)
        
        if wording_text:
            st.success(f"‚úÖ Extracted {len(wording_text)} characters from policy wording")
            st.session_state['policy_wording_text'] = wording_text
            
            with st.expander("üîé View Policy Wording Extracted Text"):
                st.text_area("Policy Wording Text", wording_text, height=300, key="wording_text")
        else:
            st.error("‚ùå Could not extract text from policy wording document")

st.markdown("---")

# Process both documents
if st.button(" Extract Information from Both Documents", type="primary", use_container_width=True):
    
    # Process Policy Schedule
    if policy_file:
        st.subheader("üìã Processing Policy Schedule...")
        with st.spinner("üîÑ Extracting text from policy schedule..."):
            policy_text = extract_text_from_file(policy_file)
        
        if policy_text:
            with st.expander("üîé View Policy Schedule Extracted Text"):
                st.text_area("Policy Text", policy_text, height=200, key="policy_text")
            
            with st.spinner("ü§ñ Processing policy with Gemini AI..."):
                policy_json, policy_error = process_policy_schedule(policy_text)
            
            if policy_json:
                st.success("‚úÖ Policy schedule information extracted successfully!")
                st.json(policy_json)
                
                st.session_state['policy_json'] = policy_json
                
                policy_json_str = json.dumps(policy_json, indent=4)
                st.download_button(
                    label="üíæ Download Policy JSON",
                    data=policy_json_str,
                    file_name=f"{policy_file.name.rsplit('.', 1)[0]}_policy.json",
                    mime="application/json",
                    key="policy_download"
                )
            else:
                st.error("‚ùå Failed to extract policy information")
                with st.expander("View Error Details"):
                    st.code(policy_error)
        else:
            st.error("‚ùå Could not extract text from policy schedule")
    else:
        st.warning("‚ö†Ô∏è Please upload a policy schedule document")
    
    st.markdown("---")
    
    # Process Claim Document
    if claim_file:
        st.subheader(" Processing Claim Document...")
        with st.spinner("üîÑ Extracting text from claim document..."):
            claim_text = extract_text_from_file(claim_file)
        
        if claim_text:
            with st.expander("üîé View Claim Document Extracted Text"):
                st.text_area("Claim Text", claim_text, height=200, key="claim_text")
            
            with st.spinner("ü§ñ Processing claim with Gemini AI..."):
                claim_json, claim_error = process_claim_document(claim_text)
            
            if claim_json:
                st.success("‚úÖ Claim document information extracted successfully!")
                st.json(claim_json)
                
                st.session_state['claim_json'] = claim_json
                
                claim_json_str = json.dumps(claim_json, indent=4)
                st.download_button(
                    label="üíæ Download Claim JSON",
                    data=claim_json_str,
                    file_name=f"{claim_file.name.rsplit('.', 1)[0]}_claim.json",
                    mime="application/json",
                    key="claim_download"
                )
            else:
                st.error("‚ùå Failed to extract claim information")
                with st.expander("View Error Details"):
                    st.code(claim_error)
        else:
            st.error("‚ùå Could not extract text from claim document")
    else:
        st.warning("‚ö†Ô∏è Please upload a claim document")

# Validation Section
st.markdown("---")
st.header("üîç Document Validation")

if st.session_state.get('policy_json') and st.session_state.get('claim_json'):
    if st.button("üîé Validate Policy & Claim Match", type="primary", use_container_width=True):
        with st.spinner("üîÑ Running validation checks..."):
            validation_results = validate_documents(
                st.session_state['policy_json'], 
                st.session_state['claim_json']
            )
        
        st.markdown("---")
        st.subheader(" Validation Report")
        display_validation_results(validation_results)
        
        if validation_results["overall_status"] == "PASS":
            st.session_state['validation_passed'] = True
            st.success("‚úÖ Validation PASSED! You can now proceed with Policy Wording Analysis.")
        else:
            st.session_state['validation_passed'] = False
            st.error("‚ùå Validation FAILED! Please resolve issues before proceeding with Policy Wording Analysis.")
else:
    st.info("‚ÑπÔ∏è Please extract information from both Policy Schedule and Claim Document to enable validation.")

# Policy Wording Analysis Section with RAG
st.markdown("---")
st.header(" Policy Wording vs Claim Analysis (RAG-Powered)")

if st.session_state.get('validation_passed', False) and rag_system:
    if st.session_state.get('policy_wording_text') and st.session_state.get('claim_json'):
        st.success("‚úÖ All prerequisites met. Ready for policy wording analysis with RAG.")
        
        if st.button("üîç Analyze Claim Against Policy Wording (RAG)", type="primary", use_container_width=True):
            with st.spinner("üîÑ Analyzing claim against policy terms using RAG..."):
                analysis_result, error = analyze_claim_with_rag(
                    st.session_state['claim_json'],
                    st.session_state['policy_wording_text'],
                    rag_system,
                    call_gemini_api
                )
            
            if analysis_result:
                st.markdown("---")
                st.subheader(" RAG-Powered Policy Analysis Report")
                display_rag_analysis(analysis_result)
                
                # Store in session
                st.session_state['rag_analysis'] = analysis_result
                # Extract doc_id from the analysis process
                # Get the most recent document added
                docs = rag_system.list_documents()
                if docs:
                    st.session_state['doc_id'] = docs[0]['id']
                    st.session_state['show_chatbot'] = True
                st.success("‚úÖ Analysis complete! Scroll down to chat with the policy assistant.")
            else:
                st.error(f"‚ùå Analysis failed: {error}")
        
        # CHATBOT SECTION - OUTSIDE THE BUTTON
        if st.session_state.get('show_chatbot') and st.session_state.get('doc_id') and rag_system:
            policy_assistant_chatbot(
                rag_system=rag_system, 
                gemini_api_function=call_gemini_api,
                document_id=st.session_state['doc_id']
            )
    else:
        st.warning("‚ö†Ô∏è Please upload and extract Policy Wording document to proceed with analysis.")
elif not rag_system:
    st.error("‚ùå RAG system not initialized. Check PostgreSQL connection.")
else:
    st.error("üö´ Policy Wording Analysis is LOCKED")
    st.info("üìå To unlock Policy Wording Analysis:")
    st.markdown("""
    1. ‚úÖ Upload and extract **Policy Schedule**
    2. ‚úÖ Upload and extract **Claim Document**
    3. ‚úÖ Run **Validation** and ensure it **PASSES**
    4. ‚úÖ Upload and extract **Policy Wording**
    5. ‚úÖ Then you can analyze claim against policy wording
    """)

st.markdown("---")
st.markdown("""
### üìù Tips for Best Results:
- **Images**: Use clear, high-resolution scans with good contrast
- **PDFs**: Both text-based and scanned PDFs are supported
- **Quality**: Ensure text is readable and not blurry
- **RAG System**: Policy wording is automatically chunked and stored for semantic search
- **Database**: PostgreSQL with pgvector extension required for RAG functionality
- **Chatbot**: After analysis, use the chatbot to ask specific questions about the policy
""")